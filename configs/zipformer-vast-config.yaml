training:
  epochs: 100
  batch_size: 8
  save_path: "workspace/Zipformer/saves"
  train_path : "workspace/dataset/train_w2i.json"
  dev_path : "workspace/dataset/test_w2i.json"
  test_path : "workspace/dataset/test_w2i.json"
  vocab_path : "workspace/dataset/vocab_w2i.json"
  log_path: "workspace/Zipformer/logs"
  result: "workspace/Zipformer/result.txt"
  tokenizer-level: "char" # word/char/phoneme
  early_stop: 5
  reload: False

model:
  conv_embeded: 
    input_dim: 80
    num_blocks: 2
    num_layers_per_block: 1
    out_channels: [64, 32]
    kernel_sizes: [3,3]
    strides: [1,2]
    residuals: [False, False]
    dropout: 0.1
  enc:
    output_downsampling_factor: 2
    # downsampling_factor: [2, 2, 2, 2, 2, 2]
    # encoder_dim: [192,256,256,256,256,256]
    downsampling_factor: [2,2,3,4,3,2]
    encoder_dim: [192,256,384,512,384,256]
    num_encoder_layers: [2,2,1,1,1,1]
    # encoder_unmasked_dim: [192,256,256,256,256,256]
    encoder_unmasked_dim: [192,256,384,512,384,256]
    query_head_dim: 32
    pos_head_dim: 4
    value_head_dim: 12     
    # num_heads: [4,4,4,4,4,4]  
    # feedforward_dim: [512,768,768,768,768,768]  
    # cnn_module_kernel: [15,15,15,15,15,15]     
    num_heads: [4,4,4,8,4,4] 
    feedforward_dim: [512,768,1024,1536,1024,768]  
    cnn_module_kernel: [31,31,15,15,15,31]     
    pos_dim: 192
    dropout: 0.1  
    warmup_batches: 4000.0
    causal: False
    chunk_size: [-1]
    left_context_frames: [-1]  

    # output_downsampling_factor: 2
    # downsampling_factor: [2, 2, 2, 2]
    # encoder_dim: [192,256,256,256]
    # num_encoder_layers: [2,2,1,1]
    # encoder_unmasked_dim: [192,256,256,256]
    # query_head_dim: 32
    # pos_head_dim: 4
    # value_head_dim: 12     
    # num_heads: [4,4,4,4]  
    # feedforward_dim: [512,768,768,768]  
    # cnn_module_kernel: [15,15,15,15]     
    # pos_dim: 192
    # dropout: 0.1  
    # warmup_batches: 4000.0
    # causal: False
    # chunk_size: [-1]
    # left_context_frames: [-1] 
  dec: 
    type: 'lstm'   
    embedding_size: 512
    hidden_size: 512
    output_size: 256 
    n_layers: 2
    dropout: 0.2
  joint: 
    input_size: 512
    hidden_size: 1024
  dropout: 0.1
  name : 'Zipformer'
  share_weight: False

optim:
  # type: sgd
  # lr: 0.0005
  # momentum: 0.9
  # weight_decay: 0
  # begin_to_adjust_lr: 60
  # nesterov: None
  # decay_rate: 0.5

  type: adam
  lr: 0.001
  weight_decay: 0.0001
  decay_rate: 0.5

scheduler:
  lr_initial: 0.001
  n_warmup_steps: 15000

rnnt_loss:
  blank: 4
  reduction: "mean" 